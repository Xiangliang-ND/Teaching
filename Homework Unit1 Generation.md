# Homework Unit 1: Text Generation with GPT-2

**Due Date:**  11:59pm on Feb 10 (Tuesday)
**Task type:** Text generation using different decoding methods

---

##  Learning Objectives
Use a pre-trained language model (GPT-2) to understand how different **decoding settings** affect text generation. You will experiment with:
- **Greedy decoding** (deterministic)
- **Top-k sampling** (constrained randomness)
- **Top-p sampling** (dynamic vocabulary cutoff)
- **Temperature scaling** (distribution sharpening/smoothing)

You will also use metrics to measure the text generated by different decoding settings, including:
- **Unique token ratio** (measures lexical diversity)
- **Unique bigram ratio** (measures the complexity and creativity of the generated text)
- **Repetition ratio** (measures the repetitiveness of generated tokens)
---

## Prerequisites

### Prep Reading
- **The Illustrated GPT-2** by Jay Alammar: https://jalammar.github.io/illustrated-gpt2/

### Environment Setup
- **Google Colab** (recommended): No local setup required, but need to change the runtime type to GPU
- **Local machine**: Python 3.10+, PyTorch, and `transformers` library

---

## Provided Materials

You are provided with a reference code availiable on [Google Colab](https://colab.research.google.com/drive/1xgOt9dRLDw8zsevOjXG4BwWTASIbFfaU?usp=sharing) that includes:
1. Model loading code
2. A `GenConfig` dataclass for configuring generation parameters
3. A `generate_text()` helper function
4. Simple diversity/repetition metrics
5. An interactive sandbox (optional, for Colab/Jupyter) to play around with different parameters for text generation

**You are allowed to use an LLM (ChatGPT, Claude, Gemini, etc.) to help understand concepts or debug code. Please disclose whether you used one and which LLM you used.**

---

## Submission

Your submission should include:
1. **Google Colab link** (with access granted) or **Jupyter Notebook** file (.ipynb)
2. **Written responses** to all questions (can be in the notebook as markdown cells)
3. **Generated outputs** for each experiment (e.g., displayed in the Google Colab)

---

## Part 1: Qualitative Analysis for Understanding Decoding Strategies (40 pts)

### Q1 (20 pts) — Greedy vs. Sampling Comparison

Using the provided prompt (or one of your own choosing), generate text with:
- **Greedy decoding** (`do_sample=False`)
- **Top-k sampling** (`top_k=40, temperature=0.9`)
- **Nucleus sampling** (`top_p=0.9, temperature=0.9`)

For each output, report:
- The generated text
- Your qualitative assessment: Is it diverse? Complex or creative? Repetitive?

**Discussion:** In 3–5 sentences, explain the key differences you observed between greedy and sampling-based methods.

### Q2 (10 pts) — Temperature Effects

Generate text using the **same prompt** with nucleus sampling (`top_p=0.9`) at three different temperatures:
- Low temperature: `temperature=0.3`
- Medium temperature: `temperature=0.9`
- High temperature: `temperature=1.5`

For each output, report:
- The generated text
- Observations about vocabulary diversity, complexity/creativity, and repetitiveness

**Discussion:** How does temperature affect the trade-off between "safe/predictable" and "creative/risky" outputs?

### Q3 (10 pts) — Intuition for Different Parameters

Without running code, predict what would happen if you set:
- `top_k=1` (with `do_sample=True`)
- `top_p=0.1` (very low nucleus)
- `temperature=0.01`

Then verify your predictions experimentally. Were you correct? Explain if there is any unexpected output.

---

## Part 2: Quantitative Analysis (40 pts)

### Q4 (25 pts) — Diversity Metrics

Using the provided `simple_metrics()` function, compute the following for each decoding strategy (Greedy, Top-k, Top-p, High-Temperature):
- **Type-token ratio** (unique words / total words)
- **Unique bigram ratio** (unique word pairs / total pairs)
- **Repetition ratio** (repeated tokens / total tokens)
- **Number of tokens** (count of number of tokens)

Present your results in a table like:

| Strategy | Type-Token Ratio | Unique Bigram Ratio | Repetition Ratio | Num Tokens |
|----------|------------------|---------------------|------------------|
| Greedy | | | | |
| Top-k (k=40) | | | | |
| Top-p (p=0.9) | | | | |
| High-Temp (1.4) | | | | |

**Discussion:** Which strategy produces the most diverse text? Which produces the most repetitive? Do the metrics align with your qualitative observations from Part 1?

### Q5 (15 pts) — Repetition Mitigation

Find or create a prompt that causes **greedy decoding to produce repetitive text** (loops or repeated phrases).

Then, attempt to fix the repetition using:
1. `repetition_penalty=1.2`
2. `no_repeat_ngram_size=3`
3. Switching to nucleus sampling (`top_p=0.9`)

Report:
- Your original prompt and the repetitive greedy output
- The output after applying each mitigation technique
- Which technique worked best, and why you think that is

---

## Part 3: Reflection (20 pts)

**(Required, 6-10 sentences total)**

If you  are deploying a text generation system, which configuration would you choose under each principle?

**(a) Creativity-consideration:** Which decoding strategy would you use if generating novel, engaging, and diverse content is the primary goal? Briefly justify.

**(b) Controllability-consideration:** Which decoding strategy offers the most predictable and reproducible outputs? When might this be important?

**(c) Application:** If you are building a chatbot for **one** application domain from the list below (you could also think about a domain of interest of your own):
- Medical Q&A chatbot
- Creative story writing assistant
- Code completion tool
- Customer service bot
- News article summarization
- ...

What decoding configuration will you choose for your chatbot? Please discuss:
- Your recommended `GenConfig` settings (temperature, top_k, top_p, repetition_penalty, etc.)
- **Justification** (2-4 sentences): Why are these settings appropriate for your chatbot? What trade-offs did you consider (e.g., creativity, consistency vs. diversity)?
